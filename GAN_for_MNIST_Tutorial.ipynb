{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrUqXYEepD0o"
      },
      "source": [
        "#### <b>GAN 실습</b>\n",
        "\n",
        "* 논문 제목: Generative Adversarial Networks <b>(NIPS 2014)</b>\n",
        "* 가장 기본적인 GAN 모델을 학습해보는 실습을 진행합니다.\n",
        "* 학습 데이터셋: <b>MNIST</b> (1 X 28 X 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEx96DYOpdAK"
      },
      "source": [
        "#### <b>필요한 라이브러리 불러오기</b>\n",
        "\n",
        "* 실습을 위한 PyTorch 라이브러리를 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CiRb7M3naHyo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from PIL import Image as img\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \"\"\"Utilities.\n",
        "# \"\"\"\n",
        "# import random\n",
        "\n",
        "# import torch\n",
        "# from torchvision.transforms.functional import resize, to_tensor, normalize, to_pil_image\n",
        "\n",
        "# from PIL import Image\n",
        "\n",
        "\n",
        "# MEAN = (0.485, 0.456, 0.406)\n",
        "# STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "\n",
        "# def set_seed(seed=None):\n",
        "#     \"\"\"Sets the random seed.\n",
        "#     \"\"\"\n",
        "#     random.seed(seed)\n",
        "#     torch.manual_seed(seed)\n",
        "#     torch.backends.cudnn.deterministic = True\n",
        "#     torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# def set_device(device=None):\n",
        "#     \"\"\"Sets the device.\n",
        "\n",
        "#     by default sets to gpu.\n",
        "#     \"\"\"\n",
        "#     return torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "# def prep_img(image: str, size=None, mean=MEAN, std=STD):\n",
        "#     \"\"\"Preprocess image.\n",
        "#     1) load as PIl\n",
        "#     2) resize\n",
        "#     3) convert to tensor\n",
        "#     4) normalize\n",
        "#     \"\"\"\n",
        "#     im = Image.open(image)\n",
        "#     size = size or im.size[::-1]\n",
        "#     texture = resize(im, size)\n",
        "#     texture_tensor = to_tensor(texture).unsqueeze(0)\n",
        "#     texture_tensor = normalize(texture_tensor, mean=mean, std=std)\n",
        "#     return texture_tensor\n",
        "\n",
        "\n",
        "# def denormalize(tensor: torch.Tensor, mean=MEAN, std=STD, inplace: bool = False):\n",
        "#     \"\"\"Based on torchvision.transforms.functional.normalize.\n",
        "#     \"\"\"\n",
        "#     tensor = tensor.clone() if not inplace else tensor\n",
        "#     mean = torch.as_tensor(mean, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)\n",
        "#     std = torch.as_tensor(std, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)\n",
        "#     tensor.mul_(std).add_(mean)\n",
        "#     return tensor\n",
        "\n",
        "\n",
        "# def to_pil(tensor: torch.Tensor):\n",
        "#     \"\"\"Converts tensor to PIL Image.\n",
        "\n",
        "#     Args:\n",
        "#         tensor (torch.Temsor): input tensor to be converted to PIL Image of torch.Size([C, H, W]).\n",
        "#     Returns:\n",
        "#         PIL Image: converted img.\n",
        "#     \"\"\"\n",
        "#     img = tensor.clone().detach().cpu()\n",
        "#     img = denormalize(img).clip(0, 1)\n",
        "#     img = to_pil_image(img)\n",
        "#     return img\n",
        "\n",
        "\n",
        "# def to_img(tensor):\n",
        "#     \"\"\"To image tensor.\n",
        "#     \"\"\"\n",
        "#     img = tensor.clone().detach().cpu()\n",
        "#     img = denormalize(img).clip(0, 1)\n",
        "#     img = img.permute(1, 2, 0)\n",
        "#     return img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp4hbA95pihv"
      },
      "source": [
        "#### <b>생성자(Generator) 및 판별자(Discriminator) 모델 정의</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Hj5al6cTZES1"
      },
      "outputs": [],
      "source": [
        "latent_dim = 100\n",
        "\n",
        "\n",
        "# 생성자(Generator) 클래스 정의\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # 하나의 블록(block) 정의\n",
        "        def block(input_dim, output_dim, normalize=True):\n",
        "            layers = [nn.Linear(input_dim, output_dim)]\n",
        "            if normalize:\n",
        "                # 배치 정규화(batch normalization) 수행(차원 동일)\n",
        "                layers.append(nn.BatchNorm1d(output_dim, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        # 생성자 모델은 연속적인 여러 개의 블록을 가짐\n",
        "        self.model = nn.Sequential(\n",
        "            *block(latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, 3 * 28 * 28),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.size(0), 3, 28, 28)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "\n",
        "# class Generator(torch.nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#         # Convolution, Batch Normalization, ReLU 연산을 합친 함수\n",
        "#         def CBR2d(input_channel, output_channel, kernel_size=3, stride=1, padding=1):\n",
        "#             layer = nn.Sequential(\n",
        "#                 nn.Conv2d(input_channel, output_channel, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "#                 nn.BatchNorm2d(num_features=output_channel),\n",
        "#                 nn.ReLU()\n",
        "#             )\n",
        "#             return layer\n",
        "\n",
        "#         # Down Path ######################\n",
        "#         # Contracting path\n",
        "#         # conv 기본적으로 kernel size 3*3 에 stride 1으로 ■■■□□□ □■■■□□ □□■■■□ □□□■■■ =>2칸씩 크기가 줄어든다\n",
        "#         # 256x256x2 => 256x256x32\n",
        "#         self.conv1 = nn.Sequential(\n",
        "#             CBR2d(3, 32, 3, 1),\n",
        "#             CBR2d(32, 32, 3, 1)\n",
        "#         )\n",
        "#         # 256x256x32 => 128x128x32\n",
        "#         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#         # 128x128x32 => 128x128x64\n",
        "#         self.conv2 = nn.Sequential(\n",
        "#             CBR2d(32, 64, 3, 1),\n",
        "#             CBR2d(64, 64, 3, 1)\n",
        "#         )\n",
        "#         # 128x128x64 => 64x64x64\n",
        "#         self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#         # 64x64x64 => 64x64x128\n",
        "#         self.conv3 = nn.Sequential(\n",
        "#             CBR2d(64, 128, 3, 1),\n",
        "#             CBR2d(128, 128, 3, 1)\n",
        "#         )\n",
        "#         # 64x64x128 => 32x32x128\n",
        "#         self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#         # 32x32x128 => 32x32x256\n",
        "#         # Contracting path 마지막에 Dropout 적용\n",
        "#         self.conv4 = nn.Sequential(\n",
        "#             CBR2d(128, 256, 3, 1),\n",
        "#             CBR2d(256, 256, 3, 1),\n",
        "#             nn.Dropout(p=0.5)\n",
        "#         )\n",
        "#         # 32x32x256 => 16x16x256\n",
        "#         self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "#         # Contracting path 끝\n",
        "#         ###################################\n",
        "\n",
        "#         # Bottlneck 구간 #########512###########\n",
        "#         # 16x16x256 => 16x16x512\n",
        "#         self.bottleNeck = nn.Sequential(\n",
        "#             CBR2d(256, 512, 3, 1),\n",
        "#             CBR2d(521, 521, 3, 1),\n",
        "#         )\n",
        "#         # Bottlneck 구간 끝\n",
        "#         ###################################\n",
        "\n",
        "#         # Up Path #########################\n",
        "#         # Expanding path\n",
        "#         # channel 수를 감소 시키며 Up-Convolution\n",
        "#         # 16x16x512 => 32x32x256\n",
        "#         self.upconv1 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2, stride=2)\n",
        "\n",
        "#         # Up-Convolution 이후 channel = 256\n",
        "#         # Contracting path 중 같은 단계의 Feature map을 가져와 Up-Convolution 결과의 Feature map과 Concat 연산\n",
        "#         # => channel = 512 가 됩니다.\n",
        "#         # forward 부분을 참고해주세요\n",
        "#         # 32x32x512 => 32x32x256\n",
        "#         self.ex_conv1 = nn.Sequential(\n",
        "#             CBR2d(512, 256, 3, 1),\n",
        "#             CBR2d(256, 256, 3, 1)\n",
        "#         )\n",
        "\n",
        "#         # 32x32x256 => 64x64x128\n",
        "#         self.upconv2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2)\n",
        "\n",
        "#         # 64x64x256 => 64x64x128\n",
        "#         self.ex_conv2 = nn.Sequential(\n",
        "#             CBR2d(256, 128, 3, 1),\n",
        "#             CBR2d(128, 128, 3, 1)\n",
        "#         )\n",
        "\n",
        "#         # 64x64x128 => 128x128x64\n",
        "#         self.upconv3 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2)\n",
        "\n",
        "#         # 128x128x128 => 128x128x64\n",
        "#         self.ex_conv3 = nn.Sequential(\n",
        "#             CBR2d(128, 64, 3, 1),\n",
        "#             CBR2d(64, 64, 3, 1)\n",
        "#         )\n",
        "\n",
        "#         # 128x128x64 => 256x256x32\n",
        "#         self.upconv4 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=2, stride=2)\n",
        "\n",
        "#         # 256x256x64 => 256x256x32\n",
        "#         self.ex_conv4 = nn.Sequential(\n",
        "#             CBR2d(64, 32, 3, 1),\n",
        "#             CBR2d(32, 32, 3, 1),\n",
        "\n",
        "#         )\n",
        "\n",
        "#         # 논문 구조상 output = 2 channel\n",
        "#         # train 데이터에서 세포 / 배경을 검출하는것이 목표여서 class_num = 1로 지정\n",
        "#         # 256x256x32 => 256x256x1\n",
        "#         self.fc = nn.Conv2d(32, 3, kernel_size=1, stride=1)\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Contracting path\n",
        "#         # 572x572x1 => 568x568x64\n",
        "#         layer1 = self.conv1(x)\n",
        "\n",
        "#         # Max Pooling\n",
        "#         # 568x568x64 => 284x284x64\n",
        "#         out = self.pool1(layer1)\n",
        "\n",
        "#         # 284x284x64 => 280x280x128\n",
        "#         layer2 = self.conv2(out)\n",
        "\n",
        "#         # Max Pooling\n",
        "#         # 280x280x128 => 140x140x128\n",
        "#         out = self.pool2(layer2)\n",
        "\n",
        "#         # 140x140x128 => 136x136x256\n",
        "#         layer3 = self.conv3(out)\n",
        "\n",
        "#         # Max Pooling\n",
        "#         # 136x136x256 => 68x68x256\n",
        "#         out = self.pool3(layer3)\n",
        "\n",
        "#         # 68x68x256 => 64x64x512\n",
        "#         layer4 = self.conv4(out)\n",
        "\n",
        "#         # Max Pooling\n",
        "#         # 64x64x512 => 32x32x512\n",
        "#         out = self.pool4(layer4)\n",
        "\n",
        "#         # bottleneck\n",
        "#         # 32x32x512 => 28x28x1024\n",
        "#         bottleNeck = self.bottleNeck(out)\n",
        "\n",
        "#         # Expanding path\n",
        "#         # 28x28x1024 => 56x56x512\n",
        "#         upconv1 = self.upconv1(bottleNeck)\n",
        "\n",
        "#         # Contracting path 중 같은 단계의 Feature map을 가져와 합침\n",
        "#         # Up-Convolution 결과의 Feature map size 만큼 CenterCrop 하여 Concat 연산\n",
        "#         # 56x56x512 => 56x56x1024\n",
        "#         cat1 = torch.cat((transforms.CenterCrop((upconv1.shape[2], upconv1.shape[3]))(layer4), upconv1), dim=1)\n",
        "#         # 레이어 4를 중간 기준으로 upconv1 의 h(upconv1.shape[2]),w(upconv1.shape[3]) 만큼 잘라서 □■ 나란히 연결\n",
        "\n",
        "#         # 56x56x1024 => 52x52x512\n",
        "#         ex_layer1 = self.ex_conv1(cat1)\n",
        "\n",
        "#         # 52x52x512 => 104x104x256\n",
        "#         upconv2 = self.upconv2(ex_layer1)\n",
        "\n",
        "#         # Contracting path 중 같은 단계의 Feature map을 가져와 합침\n",
        "#         # Up-Convolution 결과의 Feature map size 만큼 CenterCrop 하여 Concat 연산\n",
        "#         # 104x104x256 => 104x104x512\n",
        "#         cat2 = torch.cat((transforms.CenterCrop((upconv2.shape[2], upconv2.shape[3]))(layer3), upconv2), dim=1)\n",
        "#         # 레이어 3를 중간 기준으로 upconv2 의 h(upconv2.shape[2]),w(upconv2.shape[3]) 만큼 잘라서 □■ 나란히 연결\n",
        "\n",
        "#         # 104x104x512 => 100x100x256\n",
        "#         ex_layer2 = self.ex_conv2(cat2)\n",
        "\n",
        "#         # 100x100x256 => 200x200x128\n",
        "#         upconv3 = self.upconv3(ex_layer2)\n",
        "\n",
        "#         # Contracting path 중 같은 단계의 Feature map을 가져와 합침\n",
        "#         # Up-Convolution 결과의 Feature map size 만큼 CenterCrop 하여 Concat 연산\n",
        "#         # 200x200x128 => 200x200x256\n",
        "#         cat3 = torch.cat((transforms.CenterCrop((upconv3.shape[2], upconv3.shape[3]))(layer2), upconv3), dim=1)\n",
        "#         # 레이어 2를 중간 기준으로 upconv3 의 h(upconv3.shape[2]),w(upconv3.shape[3]) 만큼 잘라서 □■ 나란히 연결\n",
        "\n",
        "#         # 200x200x256 => 196x196x128\n",
        "#         ex_layer3 = self.ex_conv3(cat3)\n",
        "\n",
        "#         # 196x196x128=> 392x392x64\n",
        "#         upconv4 = self.upconv4(ex_layer3)\n",
        "\n",
        "#         # Contracting path 중 같은 단계의 Feature map을 가져와 합침\n",
        "#         # Up-Convolution 결과의 Feature map size 만큼 CenterCrop 하여 Concat 연산\n",
        "#         # 392x392x64 => 392x392x128\n",
        "#         cat4 = torch.cat((transforms.CenterCrop((upconv4.shape[2], upconv4.shape[3]))(layer1), upconv4), dim=1)\n",
        "#         # 레이어 1를 중간 기준으로 upconv4 의 h(upconv4.shape[2]),w(upconv4.shape[3]) 만큼 잘라서 □■ 나란히 연결\n",
        "\n",
        "#         # 392x392x128 => 388x388x64\n",
        "#         out = self.ex_conv4(cat4)\n",
        "\n",
        "#         # 388x388x64 => 388x388x1\n",
        "#         out = self.fc(out)\n",
        "        \n",
        "#         out2 = nn.Tanh(out)\n",
        "#         return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M_kvtvOhaLX6"
      },
      "outputs": [],
      "source": [
        "# 판별자(Discriminator) 클래스 정의\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(3 * 28 * 28, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    # 이미지에 대한 판별 결과를 반환\n",
        "    def forward(self, img):\n",
        "        flattened = img.view(img.size(0), -1)\n",
        "        output = self.model(flattened)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOilX0rBqJXn"
      },
      "source": [
        "#### <b>학습 데이터셋 불러오기</b>\n",
        "\n",
        "* 학습을 위해 MNIST 데이터셋을 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrhXIwtAqM7H",
        "outputId": "04302ab3-dc1e-4974-b0db-942fa5998fc2"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "Found no valid file for the classes MNIST. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32me:\\Project_Ch00cy\\CGVR\\TextureSynthesis\\deep-InterpolationIMG\\GAN_for_MNIST_Tutorial.ipynb Cell 10\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Project_Ch00cy/CGVR/TextureSynthesis/deep-InterpolationIMG/GAN_for_MNIST_Tutorial.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m transforms_train \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Project_Ch00cy/CGVR/TextureSynthesis/deep-InterpolationIMG/GAN_for_MNIST_Tutorial.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     transforms\u001b[39m.\u001b[39mResize(\u001b[39m28\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Project_Ch00cy/CGVR/TextureSynthesis/deep-InterpolationIMG/GAN_for_MNIST_Tutorial.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Project_Ch00cy/CGVR/TextureSynthesis/deep-InterpolationIMG/GAN_for_MNIST_Tutorial.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     transforms\u001b[39m.\u001b[39mNormalize([\u001b[39m0.5\u001b[39m], [\u001b[39m0.5\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Project_Ch00cy/CGVR/TextureSynthesis/deep-InterpolationIMG/GAN_for_MNIST_Tutorial.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m ])\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Project_Ch00cy/CGVR/TextureSynthesis/deep-InterpolationIMG/GAN_for_MNIST_Tutorial.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# train_dataset = datasets.MNIST(root=\"./dataset\", train=True, download=True, transform=transforms_train)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Project_Ch00cy/CGVR/TextureSynthesis/deep-InterpolationIMG/GAN_for_MNIST_Tutorial.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mImageFolder(root\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m, transform\u001b[39m=\u001b[39;49mtransforms)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Project_Ch00cy/CGVR/TextureSynthesis/deep-InterpolationIMG/GAN_for_MNIST_Tutorial.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m dataloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(train_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\chy\\anaconda3\\envs\\py3.10\\lib\\site-packages\\torchvision\\datasets\\folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    302\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    303\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ):\n\u001b[1;32m--> 309\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    310\u001b[0m         root,\n\u001b[0;32m    311\u001b[0m         loader,\n\u001b[0;32m    312\u001b[0m         IMG_EXTENSIONS \u001b[39mif\u001b[39;49;00m is_valid_file \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    313\u001b[0m         transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[0;32m    314\u001b[0m         target_transform\u001b[39m=\u001b[39;49mtarget_transform,\n\u001b[0;32m    315\u001b[0m         is_valid_file\u001b[39m=\u001b[39;49mis_valid_file,\n\u001b[0;32m    316\u001b[0m     )\n\u001b[0;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples\n",
            "File \u001b[1;32mc:\\Users\\chy\\anaconda3\\envs\\py3.10\\lib\\site-packages\\torchvision\\datasets\\folder.py:145\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transform\u001b[39m=\u001b[39mtransform, target_transform\u001b[39m=\u001b[39mtarget_transform)\n\u001b[0;32m    144\u001b[0m classes, class_to_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfind_classes(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot)\n\u001b[1;32m--> 145\u001b[0m samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_dataset(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot, class_to_idx, extensions, is_valid_file)\n\u001b[0;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader \u001b[39m=\u001b[39m loader\n\u001b[0;32m    148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextensions \u001b[39m=\u001b[39m extensions\n",
            "File \u001b[1;32mc:\\Users\\chy\\anaconda3\\envs\\py3.10\\lib\\site-packages\\torchvision\\datasets\\folder.py:189\u001b[0m, in \u001b[0;36mDatasetFolder.make_dataset\u001b[1;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[39mif\u001b[39;00m class_to_idx \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[39m# prevent potential bug since make_dataset() would use the class_to_idx logic of the\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     \u001b[39m# find_classes() function, instead of using that of the find_classes() method, which\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[39m# is potentially overridden and thus could have a different logic.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe class_to_idx parameter cannot be None.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 189\u001b[0m \u001b[39mreturn\u001b[39;00m make_dataset(directory, class_to_idx, extensions\u001b[39m=\u001b[39;49mextensions, is_valid_file\u001b[39m=\u001b[39;49mis_valid_file)\n",
            "File \u001b[1;32mc:\\Users\\chy\\anaconda3\\envs\\py3.10\\lib\\site-packages\\torchvision\\datasets\\folder.py:102\u001b[0m, in \u001b[0;36mmake_dataset\u001b[1;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[39mif\u001b[39;00m extensions \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    101\u001b[0m         msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSupported extensions are: \u001b[39m\u001b[39m{\u001b[39;00mextensions\u001b[39m \u001b[39m\u001b[39mif\u001b[39;00m\u001b[39m \u001b[39m\u001b[39misinstance\u001b[39m(extensions,\u001b[39m \u001b[39m\u001b[39mstr\u001b[39m)\u001b[39m \u001b[39m\u001b[39melse\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(extensions)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(msg)\n\u001b[0;32m    104\u001b[0m \u001b[39mreturn\u001b[39;00m instances\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: Found no valid file for the classes MNIST. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp"
          ]
        }
      ],
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    transforms.Resize(28),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# train_dataset = datasets.MNIST(root=\"./dataset\", train=True, download=True, transform=transforms_train)\n",
        "train_dataset = datasets.ImageFolder(root=\"data\", transform=transforms)\n",
        "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True, num_workers=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K54Z7PNIqTkO"
      },
      "source": [
        "#### <b>모델 학습 및 샘플링</b>\n",
        "\n",
        "* 학습을 위해 생성자와 판별자 모델을 초기화합니다.\n",
        "* 적절한 하이퍼 파라미터를 설정합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9ThAQIOt-74"
      },
      "source": [
        "* 모델을 학습하면서 주기적으로 샘플링하여 결과를 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srQI5xI6ar-X",
        "outputId": "e8df65c6-acb1-4f67-d672-eedec8a65c68"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "cannot pickle 'module' object",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32me:\\Project_Ch00cy\\CGVR\\TextureSynthesis\\deep-InterpolationIMG\\GAN_for_MNIST_Tutorial.ipynb Cell 13\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Project_Ch00cy/CGVR/TextureSynthesis/deep-InterpolationIMG/GAN_for_MNIST_Tutorial.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Project_Ch00cy/CGVR/TextureSynthesis/deep-InterpolationIMG/GAN_for_MNIST_Tutorial.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Project_Ch00cy/CGVR/TextureSynthesis/deep-InterpolationIMG/GAN_for_MNIST_Tutorial.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, (imgs, _) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39;49m(dataloader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Project_Ch00cy/CGVR/TextureSynthesis/deep-InterpolationIMG/GAN_for_MNIST_Tutorial.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Project_Ch00cy/CGVR/TextureSynthesis/deep-InterpolationIMG/GAN_for_MNIST_Tutorial.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39m# 진짜(real) 이미지와 가짜(fake) 이미지에 대한 정답 레이블 생성\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Project_Ch00cy/CGVR/TextureSynthesis/deep-InterpolationIMG/GAN_for_MNIST_Tutorial.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         real \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mFloatTensor(imgs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mfill_(\u001b[39m1.0\u001b[39m) \u001b[39m# 진짜(real): 1\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Project_Ch00cy/CGVR/TextureSynthesis/deep-InterpolationIMG/GAN_for_MNIST_Tutorial.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         fake \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mFloatTensor(imgs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mfill_(\u001b[39m0.0\u001b[39m) \u001b[39m# 가짜(fake): 0\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\chy\\anaconda3\\envs\\py3.10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:441\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
            "File \u001b[1;32mc:\\Users\\chy\\anaconda3\\envs\\py3.10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\chy\\anaconda3\\envs\\py3.10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1042\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1035\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m   1043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1044\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
            "File \u001b[1;32mc:\\Users\\chy\\anaconda3\\envs\\py3.10\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\chy\\anaconda3\\envs\\py3.10\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
            "File \u001b[1;32mc:\\Users\\chy\\anaconda3\\envs\\py3.10\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
            "File \u001b[1;32mc:\\Users\\chy\\anaconda3\\envs\\py3.10\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     94\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\chy\\anaconda3\\envs\\py3.10\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
            "\u001b[1;31mTypeError\u001b[0m: cannot pickle 'module' object"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "n_epochs = 50 # 학습의 횟수(epoch) 설정\n",
        "sample_interval = 2000 # 몇 번의 배치(batch)마다 결과를 출력할 것인지 설정\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "        # 진짜(real) 이미지와 가짜(fake) 이미지에 대한 정답 레이블 생성\n",
        "        real = torch.cuda.FloatTensor(imgs.size(0), 1).fill_(1.0) # 진짜(real): 1\n",
        "        fake = torch.cuda.FloatTensor(imgs.size(0), 1).fill_(0.0) # 가짜(fake): 0\n",
        "\n",
        "        real_imgs = imgs.cuda()\n",
        "\n",
        "        \"\"\" 생성자(generator)를 학습합니다. \"\"\"\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # 랜덤 노이즈(noise) 샘플링\n",
        "        z = torch.normal(mean=0, std=1, size=(imgs.shape[0], latent_dim)).cuda()\n",
        "\n",
        "        # 이미지 생성\n",
        "        generated_imgs = generator(z)\n",
        "\n",
        "        # 생성자(generator)의 손실(loss) 값 계산\n",
        "        g_loss = adversarial_loss(discriminator(generated_imgs), real)\n",
        "\n",
        "        # 생성자(generator) 업데이트\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        \"\"\" 판별자(discriminator)를 학습합니다. \"\"\"\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # 판별자(discriminator)의 손실(loss) 값 계산\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs), real)\n",
        "        fake_loss = adversarial_loss(discriminator(generated_imgs.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        # 판별자(discriminator) 업데이트\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        done = epoch * len(dataloader) + i\n",
        "        if done % sample_interval == 0:\n",
        "            # 생성된 이미지 중에서 25개만 선택하여 5 X 5 격자 이미지에 출력\n",
        "            save_image(generated_imgs.data[:25], f\"{done}.png\", nrow=5, normalize=True)\n",
        "\n",
        "            # # Show image\n",
        "            # plt.imshow(to_pil(generated_imgs.squeeze(0)))\n",
        "            # plt.show()\n",
        "\n",
        "    # 하나의 epoch이 끝날 때마다 로그(log) 출력\n",
        "    print(f\"[Epoch {epoch}/{n_epochs}] [D loss: {d_loss.item():.6f}] [G loss: {g_loss.item():.6f}] [Elapsed time: {time.time() - start_time:.2f}s]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKhzqw6U8u-H"
      },
      "source": [
        "* 생성된 이미지 예시를 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "FeC3eMGa8vc1",
        "outputId": "3e5f76de-6279-461d-98f4-1fc65e508113"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image('92000.png')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GAN for MNIST Tutorial",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
