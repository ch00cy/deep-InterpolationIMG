{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the UNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "import keras.losses as losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "n_train_B = 400\n",
    "# n_train_A = 10\n",
    "n_test = 20\n",
    "\n",
    "train_green = []\n",
    "\n",
    "for i in range (0,n_train_B,3):\n",
    "    filename = \"E:/Project_Ch00cy/CGVR/TextureSynthesis/deep-textures-main/testingTextures/capture/draw-\" + str(i) + \".bmp\"\n",
    "\n",
    "    try:\n",
    "        image = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # BGR->RGB\n",
    "        image = cv2.resize(image,(256,256), interpolation=cv2.INTER_AREA)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "    image = np.array(image)\n",
    "    train_green.append(image)\n",
    "    \n",
    "train_green = np.array(train_green, dtype=\"float32\")\n",
    "train_green /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_purple = []\n",
    "\n",
    "for i in range (3,n_train_B+3,3):\n",
    "    filename = \"E:/Project_Ch00cy/CGVR/TextureSynthesis/deep-textures-main/testingTextures/capture/draw-\" + str(i) + \".bmp\"\n",
    "    try:\n",
    "        image = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
    "        image = cv2.resize(image,(256,256), interpolation=cv2.INTER_AREA)\n",
    "    except Exception as e:\n",
    "        print(str(e)+str(i))\n",
    "    image = np.array(image)\n",
    "    train_purple.append(image)\n",
    "    \n",
    "train_purple = np.array(train_purple, dtype=\"float32\")\n",
    "train_purple /= 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_g = tf.keras.Input(shape=(256,256,3))\n",
    "        \n",
    "conv0 = layers.Conv2D(16, (3,3), activation='relu', padding = 'same')(input_g)\n",
    "mp0 = layers.MaxPooling2D((2,2))(conv0)\n",
    "        \n",
    "conv1 = layers.Conv2D(32, (3,3), activation='relu', padding = 'same')(mp0)\n",
    "mp1 = layers.MaxPooling2D((2,2))(conv1)\n",
    "\n",
    "conv2 = layers.Conv2D(64, (3,3), activation='relu', padding = 'same')(mp1)\n",
    "mp2 = layers.MaxPooling2D((2,2))(conv2)\n",
    "        \n",
    "conv3 = layers.Conv2D(128, (3,3), activation='relu', padding = 'same')(mp2)\n",
    "mp3 = layers.MaxPooling2D((2,2))(conv3)\n",
    "        \n",
    "conv4 = layers.Conv2D(256, (3,3), activation='relu', padding = 'same')(mp3)\n",
    "mp4 = layers.MaxPooling2D((2,2))(conv4)\n",
    "        \n",
    "output_e = layers.Conv2D(512, (3,3), activation='relu', padding = 'same')(mp4)\n",
    "\n",
    "convt1 = layers.Conv2DTranspose(256, (3,3), activation='relu', padding='same')(output_e) #512아니고 256\n",
    "upsamp1 = layers.UpSampling2D((2,2))(convt1)\n",
    "skipcon1 = layers.Concatenate(axis=3)([conv4, upsamp1])\n",
    "conv6 = layers.Conv2D(256, (3,3), activation = 'relu', padding='same')(skipcon1)\n",
    "\n",
    "convt2 = layers.Conv2DTranspose(128, (3,3), activation='relu', padding='same')(conv6)\n",
    "upsamp2 = layers.UpSampling2D((2,2))(convt2)\n",
    "skipcon2 = layers.Concatenate(axis=3)([conv3, upsamp2])\n",
    "conv7 = layers.Conv2D(128, (3,3), activation = 'relu', padding='same')(skipcon2)\n",
    "\n",
    "convt3 = layers.Conv2DTranspose(64, (3,3), activation='relu', padding='same')(conv7)\n",
    "upsamp3 = layers.UpSampling2D((2,2))(convt3)\n",
    "skipcon3 = layers.Concatenate(axis=3)([conv2, upsamp3])\n",
    "conv8 = layers.Conv2D(64, (3,3), activation='relu', padding='same')(skipcon3)\n",
    "\n",
    "convt4 = layers.Conv2DTranspose(32, (3,3), activation='relu', padding='same')(conv8)\n",
    "upsamp4 = layers.UpSampling2D((2,2))(convt4)\n",
    "skipcon4 = layers.Concatenate(axis=3)([conv1, upsamp4])\n",
    "conv9 = layers.Conv2D(32, (3,3), activation='relu', padding='same')(skipcon4)\n",
    "        \n",
    "convt5 = layers.Conv2DTranspose(16, (3,3), activation='relu', padding='same')(conv9)\n",
    "upsamp5 = layers.UpSampling2D((2,2))(convt5)\n",
    "skipcon5 = layers.Concatenate(axis=3)([conv0, upsamp5])\n",
    "conv10 = layers.Conv2D(16, (3,3), activation='relu', padding='same')(skipcon5)\n",
    "\n",
    "output_p = layers.Conv2DTranspose(3, (3,3), activation='relu', padding='same')(conv10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 - 9s - loss: 0.0274 - val_loss: 0.0171 - 9s/epoch - 2s/step\n",
      "Epoch 2/100\n",
      "4/4 - 7s - loss: 0.0130 - val_loss: 0.0096 - 7s/epoch - 2s/step\n",
      "Epoch 3/100\n",
      "4/4 - 7s - loss: 0.0086 - val_loss: 0.0095 - 7s/epoch - 2s/step\n",
      "Epoch 4/100\n",
      "4/4 - 7s - loss: 0.0079 - val_loss: 0.0086 - 7s/epoch - 2s/step\n",
      "Epoch 5/100\n",
      "4/4 - 6s - loss: 0.0074 - val_loss: 0.0088 - 6s/epoch - 2s/step\n",
      "Epoch 6/100\n",
      "4/4 - 7s - loss: 0.0072 - val_loss: 0.0081 - 7s/epoch - 2s/step\n",
      "Epoch 7/100\n",
      "4/4 - 7s - loss: 0.0068 - val_loss: 0.0078 - 7s/epoch - 2s/step\n",
      "Epoch 8/100\n",
      "4/4 - 6s - loss: 0.0064 - val_loss: 0.0071 - 6s/epoch - 2s/step\n",
      "Epoch 9/100\n",
      "4/4 - 6s - loss: 0.0057 - val_loss: 0.0065 - 6s/epoch - 2s/step\n",
      "Epoch 10/100\n",
      "4/4 - 7s - loss: 0.0052 - val_loss: 0.0059 - 7s/epoch - 2s/step\n",
      "Epoch 11/100\n",
      "4/4 - 7s - loss: 0.0046 - val_loss: 0.0052 - 7s/epoch - 2s/step\n",
      "Epoch 12/100\n",
      "4/4 - 7s - loss: 0.0042 - val_loss: 0.0048 - 7s/epoch - 2s/step\n",
      "Epoch 13/100\n",
      "4/4 - 6s - loss: 0.0038 - val_loss: 0.0044 - 6s/epoch - 2s/step\n",
      "Epoch 14/100\n",
      "4/4 - 7s - loss: 0.0035 - val_loss: 0.0041 - 7s/epoch - 2s/step\n",
      "Epoch 15/100\n",
      "4/4 - 7s - loss: 0.0034 - val_loss: 0.0039 - 7s/epoch - 2s/step\n",
      "Epoch 16/100\n",
      "4/4 - 7s - loss: 0.0031 - val_loss: 0.0037 - 7s/epoch - 2s/step\n",
      "Epoch 17/100\n",
      "4/4 - 7s - loss: 0.0029 - val_loss: 0.0034 - 7s/epoch - 2s/step\n",
      "Epoch 18/100\n",
      "4/4 - 7s - loss: 0.0027 - val_loss: 0.0033 - 7s/epoch - 2s/step\n",
      "Epoch 19/100\n",
      "4/4 - 6s - loss: 0.0029 - val_loss: 0.0037 - 6s/epoch - 2s/step\n",
      "Epoch 20/100\n",
      "4/4 - 7s - loss: 0.0028 - val_loss: 0.0034 - 7s/epoch - 2s/step\n",
      "Epoch 21/100\n",
      "4/4 - 7s - loss: 0.0026 - val_loss: 0.0031 - 7s/epoch - 2s/step\n",
      "Epoch 22/100\n",
      "4/4 - 7s - loss: 0.0024 - val_loss: 0.0029 - 7s/epoch - 2s/step\n",
      "Epoch 23/100\n",
      "4/4 - 7s - loss: 0.0023 - val_loss: 0.0029 - 7s/epoch - 2s/step\n",
      "Epoch 24/100\n",
      "4/4 - 7s - loss: 0.0022 - val_loss: 0.0027 - 7s/epoch - 2s/step\n",
      "Epoch 25/100\n",
      "4/4 - 7s - loss: 0.0021 - val_loss: 0.0026 - 7s/epoch - 2s/step\n",
      "Epoch 26/100\n",
      "4/4 - 7s - loss: 0.0020 - val_loss: 0.0025 - 7s/epoch - 2s/step\n",
      "Epoch 27/100\n",
      "4/4 - 7s - loss: 0.0019 - val_loss: 0.0024 - 7s/epoch - 2s/step\n",
      "Epoch 28/100\n",
      "4/4 - 7s - loss: 0.0019 - val_loss: 0.0023 - 7s/epoch - 2s/step\n",
      "Epoch 29/100\n",
      "4/4 - 7s - loss: 0.0018 - val_loss: 0.0023 - 7s/epoch - 2s/step\n",
      "Epoch 30/100\n",
      "4/4 - 7s - loss: 0.0017 - val_loss: 0.0022 - 7s/epoch - 2s/step\n",
      "Epoch 31/100\n",
      "4/4 - 7s - loss: 0.0017 - val_loss: 0.0022 - 7s/epoch - 2s/step\n",
      "Epoch 32/100\n",
      "4/4 - 7s - loss: 0.0017 - val_loss: 0.0021 - 7s/epoch - 2s/step\n",
      "Epoch 33/100\n",
      "4/4 - 7s - loss: 0.0016 - val_loss: 0.0022 - 7s/epoch - 2s/step\n",
      "Epoch 34/100\n",
      "4/4 - 7s - loss: 0.0016 - val_loss: 0.0021 - 7s/epoch - 2s/step\n",
      "Epoch 35/100\n",
      "4/4 - 7s - loss: 0.0016 - val_loss: 0.0020 - 7s/epoch - 2s/step\n",
      "Epoch 36/100\n",
      "4/4 - 7s - loss: 0.0015 - val_loss: 0.0020 - 7s/epoch - 2s/step\n",
      "Epoch 37/100\n",
      "4/4 - 6s - loss: 0.0015 - val_loss: 0.0019 - 6s/epoch - 2s/step\n",
      "Epoch 38/100\n",
      "4/4 - 7s - loss: 0.0014 - val_loss: 0.0019 - 7s/epoch - 2s/step\n",
      "Epoch 39/100\n",
      "4/4 - 7s - loss: 0.0014 - val_loss: 0.0019 - 7s/epoch - 2s/step\n",
      "Epoch 40/100\n",
      "4/4 - 7s - loss: 0.0014 - val_loss: 0.0019 - 7s/epoch - 2s/step\n",
      "Epoch 41/100\n",
      "4/4 - 7s - loss: 0.0014 - val_loss: 0.0020 - 7s/epoch - 2s/step\n",
      "Epoch 42/100\n",
      "4/4 - 7s - loss: 0.0013 - val_loss: 0.0018 - 7s/epoch - 2s/step\n",
      "Epoch 43/100\n",
      "4/4 - 7s - loss: 0.0013 - val_loss: 0.0019 - 7s/epoch - 2s/step\n",
      "Epoch 44/100\n",
      "4/4 - 7s - loss: 0.0013 - val_loss: 0.0018 - 7s/epoch - 2s/step\n",
      "Epoch 45/100\n",
      "4/4 - 7s - loss: 0.0013 - val_loss: 0.0018 - 7s/epoch - 2s/step\n",
      "Epoch 46/100\n",
      "4/4 - 7s - loss: 0.0013 - val_loss: 0.0017 - 7s/epoch - 2s/step\n",
      "Epoch 47/100\n",
      "4/4 - 7s - loss: 0.0013 - val_loss: 0.0017 - 7s/epoch - 2s/step\n",
      "Epoch 48/100\n",
      "4/4 - 6s - loss: 0.0012 - val_loss: 0.0018 - 6s/epoch - 2s/step\n",
      "Epoch 49/100\n",
      "4/4 - 6s - loss: 0.0013 - val_loss: 0.0018 - 6s/epoch - 2s/step\n",
      "Epoch 50/100\n",
      "4/4 - 6s - loss: 0.0012 - val_loss: 0.0018 - 6s/epoch - 2s/step\n",
      "Epoch 51/100\n",
      "4/4 - 6s - loss: 0.0012 - val_loss: 0.0017 - 6s/epoch - 2s/step\n",
      "Epoch 52/100\n",
      "4/4 - 7s - loss: 0.0012 - val_loss: 0.0017 - 7s/epoch - 2s/step\n",
      "Epoch 53/100\n",
      "4/4 - 6s - loss: 0.0011 - val_loss: 0.0016 - 6s/epoch - 1s/step\n",
      "Epoch 54/100\n",
      "4/4 - 6s - loss: 0.0011 - val_loss: 0.0016 - 6s/epoch - 2s/step\n",
      "Epoch 55/100\n",
      "4/4 - 6s - loss: 0.0011 - val_loss: 0.0016 - 6s/epoch - 1s/step\n",
      "Epoch 56/100\n",
      "4/4 - 6s - loss: 0.0011 - val_loss: 0.0016 - 6s/epoch - 2s/step\n",
      "Epoch 57/100\n",
      "4/4 - 6s - loss: 0.0011 - val_loss: 0.0016 - 6s/epoch - 2s/step\n",
      "Epoch 58/100\n",
      "4/4 - 6s - loss: 0.0011 - val_loss: 0.0016 - 6s/epoch - 2s/step\n",
      "Epoch 59/100\n",
      "4/4 - 6s - loss: 0.0011 - val_loss: 0.0016 - 6s/epoch - 1s/step\n",
      "Epoch 60/100\n",
      "4/4 - 6s - loss: 0.0010 - val_loss: 0.0016 - 6s/epoch - 1s/step\n",
      "Epoch 61/100\n",
      "4/4 - 6s - loss: 0.0010 - val_loss: 0.0015 - 6s/epoch - 2s/step\n",
      "Epoch 62/100\n",
      "4/4 - 6s - loss: 0.0010 - val_loss: 0.0015 - 6s/epoch - 1s/step\n",
      "Epoch 63/100\n",
      "4/4 - 6s - loss: 9.8918e-04 - val_loss: 0.0015 - 6s/epoch - 2s/step\n",
      "Epoch 64/100\n",
      "4/4 - 6s - loss: 9.8122e-04 - val_loss: 0.0015 - 6s/epoch - 1s/step\n",
      "Epoch 65/100\n",
      "4/4 - 5s - loss: 9.6930e-04 - val_loss: 0.0015 - 5s/epoch - 1s/step\n",
      "Epoch 66/100\n",
      "4/4 - 6s - loss: 9.9993e-04 - val_loss: 0.0015 - 6s/epoch - 1s/step\n",
      "Epoch 67/100\n",
      "4/4 - 6s - loss: 9.5991e-04 - val_loss: 0.0015 - 6s/epoch - 1s/step\n",
      "Epoch 68/100\n",
      "4/4 - 5s - loss: 9.6489e-04 - val_loss: 0.0015 - 5s/epoch - 1s/step\n",
      "Epoch 69/100\n",
      "4/4 - 5s - loss: 9.5204e-04 - val_loss: 0.0015 - 5s/epoch - 1s/step\n",
      "Epoch 70/100\n",
      "4/4 - 6s - loss: 9.2384e-04 - val_loss: 0.0015 - 6s/epoch - 1s/step\n",
      "Epoch 71/100\n",
      "4/4 - 6s - loss: 9.1444e-04 - val_loss: 0.0014 - 6s/epoch - 1s/step\n",
      "Epoch 72/100\n",
      "4/4 - 6s - loss: 9.0406e-04 - val_loss: 0.0015 - 6s/epoch - 1s/step\n",
      "Epoch 73/100\n",
      "4/4 - 6s - loss: 9.4921e-04 - val_loss: 0.0015 - 6s/epoch - 2s/step\n",
      "Epoch 74/100\n",
      "4/4 - 6s - loss: 9.2807e-04 - val_loss: 0.0014 - 6s/epoch - 2s/step\n",
      "Epoch 75/100\n",
      "4/4 - 6s - loss: 8.9728e-04 - val_loss: 0.0014 - 6s/epoch - 1s/step\n",
      "Epoch 76/100\n",
      "4/4 - 6s - loss: 8.6895e-04 - val_loss: 0.0014 - 6s/epoch - 2s/step\n",
      "Epoch 77/100\n",
      "4/4 - 6s - loss: 8.8359e-04 - val_loss: 0.0015 - 6s/epoch - 1s/step\n",
      "Epoch 78/100\n",
      "4/4 - 6s - loss: 8.9330e-04 - val_loss: 0.0014 - 6s/epoch - 2s/step\n",
      "Epoch 79/100\n",
      "4/4 - 6s - loss: 8.9251e-04 - val_loss: 0.0014 - 6s/epoch - 1s/step\n",
      "Epoch 80/100\n",
      "4/4 - 6s - loss: 8.8439e-04 - val_loss: 0.0014 - 6s/epoch - 1s/step\n",
      "Epoch 81/100\n",
      "4/4 - 6s - loss: 9.2606e-04 - val_loss: 0.0014 - 6s/epoch - 2s/step\n",
      "Epoch 82/100\n",
      "4/4 - 6s - loss: 9.0671e-04 - val_loss: 0.0014 - 6s/epoch - 1s/step\n",
      "Epoch 83/100\n",
      "4/4 - 252s - loss: 8.7950e-04 - val_loss: 0.0014 - 252s/epoch - 63s/step\n",
      "Epoch 84/100\n",
      "4/4 - 9s - loss: 8.6190e-04 - val_loss: 0.0014 - 9s/epoch - 2s/step\n",
      "Epoch 85/100\n",
      "4/4 - 9s - loss: 8.5579e-04 - val_loss: 0.0015 - 9s/epoch - 2s/step\n",
      "Epoch 86/100\n",
      "4/4 - 9s - loss: 8.5735e-04 - val_loss: 0.0014 - 9s/epoch - 2s/step\n",
      "Epoch 87/100\n",
      "4/4 - 9s - loss: 8.7332e-04 - val_loss: 0.0014 - 9s/epoch - 2s/step\n",
      "Epoch 88/100\n",
      "4/4 - 9s - loss: 8.3299e-04 - val_loss: 0.0014 - 9s/epoch - 2s/step\n",
      "Epoch 89/100\n",
      "4/4 - 8s - loss: 8.2520e-04 - val_loss: 0.0014 - 8s/epoch - 2s/step\n",
      "Epoch 90/100\n",
      "4/4 - 8s - loss: 8.0594e-04 - val_loss: 0.0014 - 8s/epoch - 2s/step\n",
      "Epoch 91/100\n",
      "4/4 - 8s - loss: 7.9871e-04 - val_loss: 0.0013 - 8s/epoch - 2s/step\n",
      "Epoch 92/100\n",
      "4/4 - 9s - loss: 7.7828e-04 - val_loss: 0.0013 - 9s/epoch - 2s/step\n",
      "Epoch 93/100\n",
      "4/4 - 8s - loss: 7.6697e-04 - val_loss: 0.0013 - 8s/epoch - 2s/step\n",
      "Epoch 94/100\n",
      "4/4 - 9s - loss: 7.5602e-04 - val_loss: 0.0013 - 9s/epoch - 2s/step\n",
      "Epoch 95/100\n",
      "4/4 - 8s - loss: 7.4975e-04 - val_loss: 0.0013 - 8s/epoch - 2s/step\n",
      "Epoch 96/100\n",
      "4/4 - 9s - loss: 7.4083e-04 - val_loss: 0.0013 - 9s/epoch - 2s/step\n",
      "Epoch 97/100\n",
      "4/4 - 8s - loss: 7.3227e-04 - val_loss: 0.0013 - 8s/epoch - 2s/step\n",
      "Epoch 98/100\n",
      "4/4 - 8s - loss: 7.3287e-04 - val_loss: 0.0013 - 8s/epoch - 2s/step\n",
      "Epoch 99/100\n",
      "4/4 - 8s - loss: 7.2205e-04 - val_loss: 0.0013 - 8s/epoch - 2s/step\n",
      "Epoch 100/100\n",
      "4/4 - 8s - loss: 7.1747e-04 - val_loss: 0.0013 - 8s/epoch - 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2268ddae440>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(inputs=input_g, outputs=output_p)\n",
    "model.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "model.fit(train_green, train_purple, validation_split=0.2, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_green = []\n",
    "\n",
    "for i in range (n_train_B+2,n_train_B+n_test+1,3):\n",
    "    filename = \"E:/Project_Ch00cy/CGVR/TextureSynthesis/deep-textures-main/testingTextures/capture/draw-\" + str(i) + \".bmp\"\n",
    "\n",
    "    try:\n",
    "        image = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (256,256), interpolation=cv2.INTER_AREA)\n",
    "    except Exception as e:\n",
    "        print(str(e)+str(i))\n",
    "    image = np.array(image)\n",
    "    test_green.append(image)\n",
    "\n",
    "test_green = np.array(test_green, dtype=\"float32\")\n",
    "test_green /= 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(test_input, test_output):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    display_list = [test_input, test_output]\n",
    "    title = ['Input Image', 'Predicted Image']\n",
    "    \n",
    "    for i in range(2):\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(display_list[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prediction = model(test_green)\n",
    "len(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "plt.imshow(test_prediction[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "generate_images(test_prediction[1],test_prediction[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "EagerTensor object has no attribute 'astype'. \n        If you are looking for numpy-related methods, please run the following:\n        from tensorflow.python.ops.numpy_ops import np_config\n        np_config.enable_numpy_behavior()\n      ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\Project_Ch00cy\\CGVR\\TextureSynthesis\\deep-textures-main\\UNET\\UNET_tensorflow.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Project_Ch00cy/CGVR/TextureSynthesis/deep-textures-main/UNET/UNET_tensorflow.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow((test_prediction[\u001b[39m1\u001b[39;49m] \u001b[39m*\u001b[39;49m \u001b[39m255\u001b[39;49m)\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39muint8))\n",
      "File \u001b[1;32mc:\\Users\\chy\\anaconda3\\envs\\forTensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:440\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[0;32m    437\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mravel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtranspose\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mreshape\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mclip\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msize\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    438\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mtolist\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    439\u001b[0m     \u001b[39m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[39;00m\n\u001b[1;32m--> 440\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m    441\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m    442\u001b[0m \u001b[39m      If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[0;32m    443\u001b[0m \u001b[39m      from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[0;32m    444\u001b[0m \u001b[39m      np_config.enable_numpy_behavior()\u001b[39m\n\u001b[0;32m    445\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39m)\n\u001b[0;32m    446\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: EagerTensor object has no attribute 'astype'. \n        If you are looking for numpy-related methods, please run the following:\n        from tensorflow.python.ops.numpy_ops import np_config\n        np_config.enable_numpy_behavior()\n      "
     ]
    }
   ],
   "source": [
    "plt.imshow((test_prediction[1] * 255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_prediction = model(test_green)\n",
    "\n",
    "i = 0\n",
    "for image in test_prediction:\n",
    "    generate_images(test_green[i], image)\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
